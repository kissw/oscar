{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "               Configuration Settings\n",
      "=======================================================\n",
      "Neural Net:     \tfusion_kdh_jaerock3\n",
      "Data Collection:\tfusion_kdh_jaerock3\n",
      "Run Neural:     \tfusion_kdh_jaerock3\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "# from keras.layers import Input, Conv2D, Flatten, Dense, Conv2DTranspose, Reshape, Activation, LeakyReLU, Dropout, BatchNormalization, MaxPooling2D, Lambda\n",
    "from keras.models import Model\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from progressbar import ProgressBar\n",
    "from image_process import ImageProcess\n",
    "import const, cv2\n",
    "from drive_data import DriveData\n",
    "from config import Config\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "config = Config.neural_net\n",
    "sys.setrecursionlimit(10**7)\n",
    "tf.compat.v1.experimental.output_all_intermediates(True)\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "gpu_config = tf.compat.v1.ConfigProto()\n",
    "gpu_config.gpu_options.allow_growth = True\n",
    "sess = tf.compat.v1.Session(config=gpu_config)\n",
    "tf.compat.v1.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home2/kdh/anaconda3/envs/vae/lib/python3.6/site-packages/keras/layers/normalization/batch_normalization.py:520: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(1, 2, 160, 160, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (1, 2, 160, 160, 24) 1824        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (1, 2, 160, 160, 24) 96          time_distributed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (1, 2, 160, 160, 24) 0           time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (1, 2, 80, 80, 24)   0           time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_4 (TimeDistrib (1, 2, 80, 80, 36)   21636       time_distributed_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_5 (TimeDistrib (1, 2, 80, 80, 36)   144         time_distributed_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_6 (TimeDistrib (1, 2, 80, 80, 36)   0           time_distributed_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_7 (TimeDistrib (1, 2, 40, 40, 36)   0           time_distributed_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_8 (TimeDistrib (1, 2, 40, 40, 48)   43248       time_distributed_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_9 (TimeDistrib (1, 2, 40, 40, 48)   192         time_distributed_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_10 (TimeDistri (1, 2, 40, 40, 48)   0           time_distributed_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_11 (TimeDistri (1, 2, 40, 40, 64)   27712       time_distributed_10[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_12 (TimeDistri (1, 2, 40, 40, 64)   256         time_distributed_11[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_13 (TimeDistri (1, 2, 40, 40, 64)   0           time_distributed_12[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_14 (TimeDistri (1, 2, 40, 40, 64)   36928       time_distributed_13[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_15 (TimeDistri (1, 2, 40, 40, 64)   256         time_distributed_14[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_16 (TimeDistri (1, 2, 40, 40, 64)   0           time_distributed_15[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_17 (TimeDistri (1, 2, 102400)       0           time_distributed_16[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(1, 1)]             0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(1, 1)]             0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(1, 1)]             0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_18 (TimeDistri (1, 2, 500)          51200500    time_distributed_17[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (1, 100)             200         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (1, 100)             200         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (1, 100)             200         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_19 (TimeDistri (1, 2, 500)          1000        time_distributed_18[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (1, 100)             0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (1, 100)             0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (1, 100)             0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_20 (TimeDistri (1, 2, 500)          0           time_distributed_19[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (1, 50)              5050        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (1, 50)              5050        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (1, 50)              5050        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (1, 1000)            4004000     time_distributed_20[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (1, 50)              0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (1, 50)              0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (1, 50)              0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (1, 1150)            0           bidirectional[0][0]              \n",
      "                                                                 activation_7[0][0]               \n",
      "                                                                 activation_9[0][0]               \n",
      "                                                                 activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (1, 500)             575500      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (1, 500)             0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (1, 100)             50100       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (1, 100)             0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (1, 50)              5050        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (1, 50)              5050        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sampling (Sampling)             (1, 50)              0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Functional)            (None, 160, 160, 3)  5288551     sampling[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 61,277,793\n",
      "Trainable params: 61,277,321\n",
      "Non-trainable params: 472\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "## Encoder\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "## Create a sampling layer\n",
    "\"\"\"\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "encoder_inputs_img = tf.keras.Input(batch_shape=(1, 1, 160, 160, 3))\n",
    "encoder_inputs_str = tf.keras.Input(batch_shape=(1, 1))\n",
    "encoder_inputs_vel = tf.keras.Input(batch_shape=(1, 1))\n",
    "encoder_inputs_time = tf.keras.Input(batch_shape=(1, 1))\n",
    "x = layers.TimeDistributed(layers.Conv2D(24, (5, 5), padding='same', name='conv2d_1'))(encoder_inputs_img)\n",
    "x = layers.TimeDistributed(layers.BatchNormalization())(x)\n",
    "x = layers.TimeDistributed(layers.Activation('elu'))(x)\n",
    "x = layers.TimeDistributed(layers.MaxPooling2D(pool_size=(2, 2), name='pool2d_1'))(x)\n",
    "# x = layers.TimeDistributed(layers.MaxPooling2D(pool_size=(2, 2), name='pool2d_1'))(x)\n",
    "x = layers.TimeDistributed(layers.Conv2D(36, (5, 5), padding='same', name='conv2d_2'))(x)\n",
    "x = layers.TimeDistributed(layers.BatchNormalization())(x)\n",
    "x = layers.TimeDistributed(layers.Activation('elu'))(x)\n",
    "x = layers.TimeDistributed(layers.MaxPooling2D(pool_size=(2, 2), name='pool2d_2'))(x)\n",
    "x = layers.TimeDistributed(layers.Conv2D(48, (5, 5), padding='same', name='conv2d_3'))(x)\n",
    "x = layers.TimeDistributed(layers.BatchNormalization())(x)\n",
    "x = layers.TimeDistributed(layers.Activation('elu'))(x)\n",
    "x = layers.TimeDistributed(layers.Conv2D(64, (3, 3), padding='same', name='conv2d_4'))(x)\n",
    "x = layers.TimeDistributed(layers.BatchNormalization())(x)\n",
    "x = layers.TimeDistributed(layers.Activation('elu'))(x)\n",
    "x = layers.TimeDistributed(layers.Conv2D(64, (3, 3), padding='same', name='conv2d_5'))(x)\n",
    "x = layers.TimeDistributed(layers.BatchNormalization())(x)\n",
    "x = layers.TimeDistributed(layers.Activation('elu'))(x)\n",
    "latent = layers.TimeDistributed(layers.Flatten())(x)\n",
    "latent = layers.TimeDistributed(layers.Dense(500))(latent)\n",
    "latent = layers.TimeDistributed(layers.LayerNormalization())(latent)\n",
    "latent = layers.TimeDistributed(layers.Activation('tanh'))(latent)\n",
    "fc_s1  = layers.Dense(100)(encoder_inputs_str)\n",
    "fc_s1  = layers.Activation('elu')(fc_s1)\n",
    "fc_s2  = layers.Dense(50)(fc_s1)\n",
    "fc_s2  = layers.Activation('elu')(fc_s2)\n",
    "fc_v1  = layers.Dense(100)(encoder_inputs_vel)\n",
    "fc_v1  = layers.Activation('elu')(fc_v1)\n",
    "fc_v2  = layers.Dense(50)(fc_v1)\n",
    "fc_v2  = layers.Activation('elu')(fc_v2)\n",
    "fc_t1  = layers.Dense(100)(encoder_inputs_time)\n",
    "fc_t1  = layers.Activation('elu')(fc_t1)\n",
    "fc_t2  = layers.Dense(50)(fc_t1)\n",
    "fc_t2  = layers.Activation('elu')(fc_t2)\n",
    "# conc_1 = layers.concatenate([latent, fc_s2, fc_v2, fc_t2])\n",
    "bilstm = layers.Bidirectional(layers.LSTM(500, batch_size=1, stateful=True))(latent)\n",
    "conc_1 = layers.concatenate([bilstm, fc_s2, fc_v2, fc_t2])\n",
    "fc_1   = layers.Dense(500)(conc_1)\n",
    "fc_1   = layers.Activation('elu')(fc_1)\n",
    "fc_2   = layers.Dense(100)(fc_1)\n",
    "x   = layers.Activation('elu')(fc_2)\n",
    "z_mean = layers.Dense(50, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(50, name=\"z_log_var\")(x)\n",
    "encoder_output = Sampling()([z_mean, z_log_var])\n",
    "encoder = tf.keras.Model([ encoder_inputs_img, encoder_inputs_str,\n",
    "                        encoder_inputs_vel, encoder_inputs_time], \n",
    "                        [z_mean, z_log_var, encoder_output], \n",
    "                        name=\"encoder\")\n",
    "\n",
    "\"\"\"\n",
    "## Decoder\n",
    "\"\"\"\n",
    "latent_inputs = tf.keras.Input(shape=(50,))\n",
    "x = layers.Dense(40 * 40 * 64, activation=\"elu\")(latent_inputs)\n",
    "x = layers.Reshape((40, 40, 64))(x)\n",
    "x = layers.Conv2DTranspose(64, 3, activation=\"elu\",  padding=\"same\")(x)\n",
    "x = layers.Conv2DTranspose(36, 3, activation=\"elu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Conv2DTranspose(24, 3, activation=\"elu\", strides=2, padding=\"same\")(x)\n",
    "decoder_outputs = layers.Conv2DTranspose(3, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
    "decoder = tf.keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "\n",
    "\"\"\"\n",
    "## VAE\n",
    "\"\"\"\n",
    "model_input = [ encoder_inputs_img, encoder_inputs_str,\n",
    "                        encoder_inputs_vel, encoder_inputs_time]\n",
    "model_output = decoder(encoder_output)\n",
    "\n",
    "VarAE=Model(model_input, model_output)\n",
    "VarAE.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home2/kdh/vae/2022-12-08-14-25-28/2022-12-08-14-25-28.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "## Dataset\n",
    "\"\"\"\n",
    "image_process = ImageProcess()\n",
    "config = Config.neural_net\n",
    "# data_path = \"/mnt/exData/internal_simulation/test/2022-03-08-17-18-14/\"\n",
    "data_path = \"/home2/kdh/vae/2022-12-08-14-25-28/\"\n",
    "if data_path[-1] == '/':\n",
    "    data_path = data_path[:-1]\n",
    "\n",
    "    loc_slash = data_path.rfind('/')\n",
    "    if loc_slash != -1: # there is '/' in the data path\n",
    "        model_name = data_path[loc_slash + 1:] # get folder name\n",
    "        #model_name = model_name.strip('/')\n",
    "    else:\n",
    "        model_name = data_path\n",
    "csv_path = data_path + '/' + model_name + const.DATA_EXT \n",
    "# data = DriveData(csv_path)\n",
    "print(csv_path)\n",
    "# data_path = data_path\n",
    "# data.read()\n",
    "\n",
    "csv_header = ['image_fname', \n",
    "                  'steering_angle', 'throttle', 'brake', 'linux_time', \n",
    "                  'vel', 'vel_x', 'vel_y', 'vel_z', \n",
    "                  'pos_x', 'pos_y', 'pos_z', \n",
    "                  'tar_image_fname', 'tar_steering_angle', 'tar_vel', 'tar_time','prev_image_fname']\n",
    "\n",
    "df = pd.read_csv(csv_path, names=csv_header, index_col=False)\n",
    "num_data = len(df)\n",
    "\n",
    "bar = ProgressBar()\n",
    "df_image_names = []\n",
    "df_prev_image_names = []\n",
    "df_measurements = []\n",
    "df_time_stamps= []\n",
    "df_velocities= []\n",
    "df_velocities_xyz= []\n",
    "df_positions_xyz= []\n",
    "df_tar_image_names= []\n",
    "df_tar_steering_angle= []\n",
    "df_tar_vel= []\n",
    "df_tar_time= []\n",
    "for i in bar(range(num_data)): # we don't have a title\n",
    "    df_image_names.append(df.loc[i]['image_fname'])\n",
    "    # if Config.data_collection['brake'] is True:\n",
    "    df_measurements.append((float(df.loc[i]['steering_angle']),\n",
    "                            float(df.loc[i]['throttle']), \n",
    "                            float(df.loc[i]['brake'])))\n",
    "    df_time_stamps.append(float(df.loc[i]['linux_time']))\n",
    "    df_velocities.append(float(df.loc[i]['vel']))\n",
    "    df_velocities_xyz.append((float(df.loc[i]['vel_x']), \n",
    "                                float(df.loc[i]['vel_y']), \n",
    "                                float(df.loc[i]['vel_z'])))\n",
    "    df_positions_xyz.append((float(df.loc[i]['pos_x']), \n",
    "                                float(df.loc[i]['pos_y']), \n",
    "                                float(df.loc[i]['pos_z'])))\n",
    "    df_tar_image_names.append(df.loc[i]['tar_image_fname'])\n",
    "    df_tar_steering_angle.append(float(df.loc[i]['tar_steering_angle']))\n",
    "    df_tar_vel.append(float(df.loc[i]['tar_vel']))\n",
    "    df_tar_time.append(float(df.loc[i]['tar_time']))\n",
    "    df_prev_image_names.append(df.loc[i]['prev_image_fname'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = list(zip( df_image_names, df_velocities, df_measurements, \n",
    "                    df_tar_image_names, df_tar_steering_angle, df_tar_vel, df_tar_time, df_prev_image_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_vae_batch_samples(batch_samples):\n",
    "    images = []\n",
    "    # prev_images = []\n",
    "    steering_angles = []\n",
    "    vels = []\n",
    "    tar_images = []\n",
    "    tar_steering_angles = []\n",
    "    tar_vels = []\n",
    "    tar_times = []\n",
    "\n",
    "\n",
    "    batch_images = []\n",
    "    # batch_prev_images = []\n",
    "    batch_steering_angles = []\n",
    "    batch_vels = []\n",
    "    batch_tar_images = []\n",
    "    batch_tar_steering_angles = []\n",
    "    batch_tar_vels = []\n",
    "    batch_tar_times = []\n",
    "        \n",
    "    for image_name, velocity, measurement, tar_image_name, tar_steering_angle, tar_vel, tar_time, prev_image_name in batch_samples:\n",
    "        # self.data.image_names, self.data.velocities, self.data.measurements, \n",
    "        # self.data.tar_image_names, self.data.tar_steering_angle, self.data.tar_vel, self.data.tar_time\n",
    "        \n",
    "        image_path = data_path + '/' + image_name\n",
    "        prev_image_path = data_path + '/' + prev_image_name\n",
    "        # print(data_path, tar_image_name)\n",
    "        tar_image_path = data_path + '/' + tar_image_name\n",
    "        image = cv2.imread(image_path)\n",
    "        prev_image = cv2.imread(prev_image_path)\n",
    "        tar_image = cv2.imread(tar_image_path)\n",
    "\n",
    "        # if collected data is not cropped then crop here\n",
    "        # otherwise do not crop.\n",
    "        if Config.data_collection['crop'] is not True:\n",
    "            image = image[Config.data_collection['image_crop_y1']:Config.data_collection['image_crop_y2'],\n",
    "                        Config.data_collection['image_crop_x1']:Config.data_collection['image_crop_x2']]\n",
    "            prev_image = prev_image[Config.data_collection['image_crop_y1']:Config.data_collection['image_crop_y2'],\n",
    "                        Config.data_collection['image_crop_x1']:Config.data_collection['image_crop_x2']]\n",
    "            tar_image = tar_image[Config.data_collection['image_crop_y1']:Config.data_collection['image_crop_y2'],\n",
    "                        Config.data_collection['image_crop_x1']:Config.data_collection['image_crop_x2']]\n",
    "        image = cv2.resize(image, \n",
    "                            (config['input_image_width'],\n",
    "                            config['input_image_height']))\n",
    "        prev_image = cv2.resize(prev_image, \n",
    "                            (config['input_image_width'],\n",
    "                            config['input_image_height']))\n",
    "        tar_image = cv2.resize(tar_image, \n",
    "                            (config['input_image_width'],\n",
    "                            config['input_image_height']))\n",
    "        image = image_process.process(image)\n",
    "        prev_image = image_process.process(prev_image)\n",
    "        tar_image = image_process.process(tar_image)\n",
    "\n",
    "        batch_vels.append(velocity)\n",
    "        batch_tar_vels.append(tar_vel)\n",
    "        batch_tar_times.append(tar_time)\n",
    "        batch_tar_steering_angles.append(tar_steering_angle)\n",
    "        # if no brake data in collected data, brake values are dummy\n",
    "        steering_angle, throttle, brake = measurement\n",
    "        batch_steering_angles.append(steering_angle)\n",
    "        # cv2.imwrite('/home2/kdh/vae/oscar/test/image/'+image_name, image)\n",
    "        # cv2.imwrite('/home2/kdh/vae/oscar/test/tar_image/'+tar_image_name, tar_image)\n",
    "        # if data == 'train':\n",
    "        #     cv2.imwrite('/mnt/Data/oscar/train_data/'+image_name, image)\n",
    "        # print(image.shape)\n",
    "        batch_images.append((image, prev_image))\n",
    "        tar_images.append(tar_image)\n",
    "        # batch_prev_images.append(prev_image)\n",
    "        # segimgs.append(segimg)\n",
    "\n",
    "\n",
    "    images.append(batch_images)\n",
    "    # prev_images.append(batch_prev_images)\n",
    "    steering_angles.append(batch_steering_angles)\n",
    "    vels.append(batch_vels)\n",
    "    # tar_images.append(batch_tar_images)\n",
    "    tar_steering_angles.append(batch_tar_steering_angles)\n",
    "    tar_vels.append(batch_tar_vels)\n",
    "    tar_times.append(batch_tar_times)\n",
    "    # print(np.array(tar_times).shape)\n",
    "    return images, vels, steering_angles, tar_images, tar_steering_angles, tar_vels, tar_times#, prev_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data generator\n",
    "\"\"\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def _generator(samples, batch_size=config['batch_size']):\n",
    "    num_samples = len(samples)\n",
    "    while True: # Loop forever so the generator never terminates\n",
    "        for offset in range(0, (num_samples//batch_size)*batch_size, batch_size):\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "\n",
    "            images, vels, steering_angles, tar_images, tar_steering_angles, tar_vels, tar_times = prepare_vae_batch_samples(batch_samples)\n",
    "\n",
    "            X_img = np.array(images).astype(\"float32\")/255.0\n",
    "            # X_prev_img = np.array(prev_images).astype(\"float32\")/255.0\n",
    "            X_tvel = np.array(tar_vels).astype(\"float32\")\n",
    "            X_tstr = np.array(tar_steering_angles).astype(\"float32\")\n",
    "            X_ttime = np.array(tar_times).astype(\"float32\")\n",
    "\n",
    "            # print(X_img.shape)\n",
    "            # print(X_tvel.shape)\n",
    "            X_img = np.squeeze(X_img, axis=0)\n",
    "            # print(X_img.shape)\n",
    "            # print(X_img.shape)\n",
    "            # X_prev_img = X_prev_img.swapaxes(0,1)\n",
    "            \n",
    "            X_tvel = X_tvel.swapaxes(0,1)\n",
    "            X_tstr = X_tstr.swapaxes(0,1)\n",
    "            X_ttime = X_ttime.swapaxes(0,1)\n",
    "            # X_tvel = X_tvel.reshape(-1, 1, 1)\n",
    "            # X_tstr = X_tstr.reshape(-1, 1, 1)\n",
    "            # X_ttime = X_ttime.reshape(-1, 1, 1)\n",
    "            y_train = np.array(tar_images).astype(\"float32\")/255.0\n",
    "            # print(X_img.shape)\n",
    "            # print(X_tvel.shape)\n",
    "            # print(y_train.shape)\n",
    "            # if config['num_outputs'] == 1:\n",
    "            # y_train = np.array(segimg)\n",
    "            # print(y_train.max())\n",
    "            # y_train = np.repeat(y_train[..., np.newaxis], 1, -1)/y_train.max()\n",
    "            X_train = [X_img, X_tstr, X_tvel, X_ttime]\n",
    "            # print(X_tstr, X_tvel, X_ttime)\n",
    "            # print(y_train)\n",
    "            # print(y_train.shape)\n",
    "            yield X_train, y_train\n",
    "\n",
    "train_data, valid_data = train_test_split(samples, test_size=config['validation_rate'])\n",
    "\n",
    "train_generator = _generator(train_data, 1)\n",
    "valid_generator = _generator(valid_data, 1)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "VAE loss\n",
    "\"\"\"\n",
    "optimizer=tf.keras.optimizers.Adam(learning_rate=0.00005, decay=0.0000001)\n",
    "# optimizer=tf.keras.optimizers.Adam(learning_rate=0.00005, decay=0.00000001)\n",
    "#                                  learning_rate=0.000005,decay=0.0000000001\n",
    "r_loss_factor=160*160*100   # This is a Hyperparameter\n",
    "\n",
    "def vae_r_loss(y_true, y_pred):    ## MSE\n",
    "    r_loss = tf.keras.backend.mean(tf.keras.backend.square(y_true-y_pred), axis=[1,2,3])\n",
    "    return r_loss_factor * r_loss\n",
    "\n",
    "def vae_kl_loss(y_true, y_pred):   ## KL-Divergence\n",
    "    kl_loss=( -0.5 * tf.keras.backend.sum(1+z_log_var \n",
    "                - tf.keras.backend.square(z_mean) \n",
    "                - tf.keras.backend.exp(z_log_var), axis=1)\n",
    "    )\n",
    "    return kl_loss\n",
    "\n",
    "def vae_loss(y_true, y_pred): \n",
    "    r_loss=vae_r_loss(y_true, y_pred) #Loss of Decoder\n",
    "    kl_loss = vae_kl_loss(y_true, y_pred) #Loss of Encoder\n",
    "    return r_loss + kl_loss #Sum of these two\n",
    "\n",
    "\n",
    "VarAE.compile(optimizer=optimizer,\n",
    "                loss= vae_loss, \n",
    "                metrics=[vae_r_loss, vae_kl_loss])\n",
    "# VarAE.summary()\n",
    "#  vae_r_loss: 12.6586 - vae_kl_loss: 2251744.5000 \n",
    "#  val_loss: 50649853.6174 \n",
    "#  val_vae_r_loss: 28.6742 - val_vae_kl_loss: 50649896.0000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "Epoch 1/100000\n",
      "111957/111958 [============================>.] - ETA: 0s - batch: 55978.0000 - size: 1.0000 - loss: 6535.9056 - vae_r_loss: 6272.2607 - vae_kl_loss: 263.5603WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/kdh/anaconda3/envs/vae/lib/python3.6/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 27960.32460, saving model to vae_kdh_ckpt.01-27960.325.h5\n",
      "111958/111958 [==============================] - 2962s 26ms/step - batch: 55978.5000 - size: 1.0000 - loss: 6535.8804 - vae_r_loss: 6272.2363 - vae_kl_loss: 263.5595 - val_loss: 27960.3246 - val_vae_r_loss: 27787.6035 - val_vae_kl_loss: 172.6376\n",
      "Epoch 2/100000\n",
      "111958/111958 [==============================] - ETA: 0s - batch: 55978.5000 - size: 1.0000 - loss: 5254.0753 - vae_r_loss: 5067.9146 - vae_kl_loss: 186.1145WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "\n",
      "Epoch 00002: val_loss improved from 27960.32460 to 25890.76313, saving model to vae_kdh_ckpt.02-25890.763.h5\n",
      "111958/111958 [==============================] - 2987s 27ms/step - batch: 55978.5000 - size: 1.0000 - loss: 5254.0753 - vae_r_loss: 5067.9146 - vae_kl_loss: 186.1145 - val_loss: 25890.7631 - val_vae_r_loss: 25755.3711 - val_vae_kl_loss: 135.2190\n",
      "Epoch 3/100000\n",
      "111957/111958 [============================>.] - ETA: 0s - batch: 55978.0000 - size: 1.0000 - loss: 4963.4363 - vae_r_loss: 4791.0078 - vae_kl_loss: 172.4102WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "\n",
      "Epoch 00003: val_loss improved from 25890.76313 to 25289.58046, saving model to vae_kdh_ckpt.03-25289.580.h5\n",
      "111958/111958 [==============================] - 3615s 32ms/step - batch: 55978.5000 - size: 1.0000 - loss: 4963.4086 - vae_r_loss: 4790.9805 - vae_kl_loss: 172.4100 - val_loss: 25289.5805 - val_vae_r_loss: 25160.3262 - val_vae_kl_loss: 129.2136\n",
      "Epoch 4/100000\n",
      "111957/111958 [============================>.] - ETA: 0s - batch: 55978.0000 - size: 1.0000 - loss: 4570.1980 - vae_r_loss: 4405.6104 - vae_kl_loss: 164.5964WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 25289.58046\n",
      "111958/111958 [==============================] - 4182s 37ms/step - batch: 55978.5000 - size: 1.0000 - loss: 4570.1707 - vae_r_loss: 4405.5830 - vae_kl_loss: 164.5965 - val_loss: 26074.7762 - val_vae_r_loss: 25944.2578 - val_vae_kl_loss: 130.5769\n",
      "Epoch 5/100000\n",
      "111958/111958 [==============================] - ETA: 0s - batch: 55978.5000 - size: 1.0000 - loss: 4743.1540 - vae_r_loss: 4578.1177 - vae_kl_loss: 165.0267WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "\n",
      "Epoch 00005: val_loss improved from 25289.58046 to 24853.26325, saving model to vae_kdh_ckpt.05-24853.263.h5\n",
      "111958/111958 [==============================] - 4169s 37ms/step - batch: 55978.5000 - size: 1.0000 - loss: 4743.1540 - vae_r_loss: 4578.1177 - vae_kl_loss: 165.0267 - val_loss: 24853.2633 - val_vae_r_loss: 24727.5156 - val_vae_kl_loss: 125.6760\n",
      "Epoch 6/100000\n",
      "111956/111958 [============================>.] - ETA: 0s - batch: 55977.5000 - size: 1.0000 - loss: 4640.4791 - vae_r_loss: 4478.1641 - vae_kl_loss: 162.3563WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 24853.26325\n",
      "111958/111958 [==============================] - 3046s 27ms/step - batch: 55978.5000 - size: 1.0000 - loss: 4640.4956 - vae_r_loss: 4478.1807 - vae_kl_loss: 162.3560 - val_loss: 25597.1984 - val_vae_r_loss: 25471.2715 - val_vae_kl_loss: 125.9409\n",
      "Epoch 7/100000\n",
      "111957/111958 [============================>.] - ETA: 0s - batch: 55978.0000 - size: 1.0000 - loss: 4326.5540 - vae_r_loss: 4167.2031 - vae_kl_loss: 159.3113WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 24853.26325\n",
      "111958/111958 [==============================] - 2977s 27ms/step - batch: 55978.5000 - size: 1.0000 - loss: 4326.5456 - vae_r_loss: 4167.1943 - vae_kl_loss: 159.3117 - val_loss: 25793.6050 - val_vae_r_loss: 25663.9258 - val_vae_kl_loss: 129.6163\n",
      "Epoch 8/100000\n",
      "111958/111958 [==============================] - ETA: 0s - batch: 55978.5000 - size: 1.0000 - loss: 4229.5926 - vae_r_loss: 4070.7324 - vae_kl_loss: 158.8617WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 24853.26325\n",
      "111958/111958 [==============================] - 2945s 26ms/step - batch: 55978.5000 - size: 1.0000 - loss: 4229.5926 - vae_r_loss: 4070.7324 - vae_kl_loss: 158.8617 - val_loss: 25821.5469 - val_vae_r_loss: 25692.6289 - val_vae_kl_loss: 128.9140\n",
      "Epoch 9/100000\n",
      "111955/111958 [============================>.] - ETA: 0s - batch: 55977.0000 - size: 1.0000 - loss: 4173.9431 - vae_r_loss: 4015.0854 - vae_kl_loss: 158.8607WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 24853.26325\n",
      "111958/111958 [==============================] - 2957s 26ms/step - batch: 55978.5000 - size: 1.0000 - loss: 4173.8613 - vae_r_loss: 4015.0037 - vae_kl_loss: 158.8607 - val_loss: 24997.0860 - val_vae_r_loss: 24876.5469 - val_vae_kl_loss: 120.3848\n",
      "Epoch 10/100000\n",
      "111956/111958 [============================>.] - ETA: 0s - batch: 55977.5000 - size: 1.0000 - loss: 4285.8794 - vae_r_loss: 4125.4312 - vae_kl_loss: 160.4372WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 24853.26325\n",
      "111958/111958 [==============================] - 2986s 27ms/step - batch: 55978.5000 - size: 1.0000 - loss: 4285.8415 - vae_r_loss: 4125.3926 - vae_kl_loss: 160.4377 - val_loss: 25847.2088 - val_vae_r_loss: 25721.9180 - val_vae_kl_loss: 125.2731\n",
      "Epoch 11/100000\n",
      "111958/111958 [==============================] - ETA: 0s - batch: 55978.5000 - size: 1.0000 - loss: 4249.8892 - vae_r_loss: 4091.1116 - vae_kl_loss: 158.7651WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 24853.26325\n",
      "111958/111958 [==============================] - 2960s 26ms/step - batch: 55978.5000 - size: 1.0000 - loss: 4249.8892 - vae_r_loss: 4091.1116 - vae_kl_loss: 158.7651 - val_loss: 25614.3190 - val_vae_r_loss: 25486.8438 - val_vae_kl_loss: 127.6695\n",
      "Epoch 12/100000\n",
      "111956/111958 [============================>.] - ETA: 0s - batch: 55977.5000 - size: 1.0000 - loss: 4281.6943 - vae_r_loss: 4122.1738 - vae_kl_loss: 159.5338WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 24853.26325\n",
      "111958/111958 [==============================] - 2976s 27ms/step - batch: 55978.5000 - size: 1.0000 - loss: 4281.6414 - vae_r_loss: 4122.1211 - vae_kl_loss: 159.5337 - val_loss: 25125.8569 - val_vae_r_loss: 25001.5273 - val_vae_kl_loss: 124.4246\n",
      "Epoch 13/100000\n",
      "111958/111958 [==============================] - ETA: 0s - batch: 55978.5000 - size: 1.0000 - loss: 4115.6400 - vae_r_loss: 3957.1453 - vae_kl_loss: 158.4807WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 24853.26325\n",
      "111958/111958 [==============================] - 3001s 27ms/step - batch: 55978.5000 - size: 1.0000 - loss: 4115.6400 - vae_r_loss: 3957.1453 - vae_kl_loss: 158.4807 - val_loss: 25339.8216 - val_vae_r_loss: 25216.9648 - val_vae_kl_loss: 122.8924\n",
      "Epoch 14/100000\n",
      "111958/111958 [==============================] - ETA: 0s - batch: 55978.5000 - size: 1.0000 - loss: 4261.2244 - vae_r_loss: 4102.9902 - vae_kl_loss: 158.2304WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 24853.26325\n",
      "111958/111958 [==============================] - 2982s 27ms/step - batch: 55978.5000 - size: 1.0000 - loss: 4261.2244 - vae_r_loss: 4102.9902 - vae_kl_loss: 158.2304 - val_loss: 25965.9931 - val_vae_r_loss: 25843.3633 - val_vae_kl_loss: 122.7053\n",
      "Epoch 15/100000\n",
      "111956/111958 [============================>.] - ETA: 0s - batch: 55977.5000 - size: 1.0000 - loss: 4123.2545 - vae_r_loss: 3965.3708 - vae_kl_loss: 157.8819WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 24853.26325\n",
      "111958/111958 [==============================] - 2994s 27ms/step - batch: 55978.5000 - size: 1.0000 - loss: 4123.2005 - vae_r_loss: 3965.3169 - vae_kl_loss: 157.8818 - val_loss: 25175.3390 - val_vae_r_loss: 25054.4043 - val_vae_kl_loss: 121.0971\n",
      "Epoch 16/100000\n",
      "111958/111958 [==============================] - ETA: 0s - batch: 55978.5000 - size: 1.0000 - loss: 4007.1743 - vae_r_loss: 3849.7402 - vae_kl_loss: 157.4416WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 24853.26325\n",
      "111958/111958 [==============================] - 2971s 27ms/step - batch: 55978.5000 - size: 1.0000 - loss: 4007.1743 - vae_r_loss: 3849.7402 - vae_kl_loss: 157.4416 - val_loss: 24922.0944 - val_vae_r_loss: 24798.3320 - val_vae_kl_loss: 123.7234\n",
      "Epoch 17/100000\n",
      "111956/111958 [============================>.] - ETA: 0s - batch: 55977.5000 - size: 1.0000 - loss: 4175.1738 - vae_r_loss: 4015.9199 - vae_kl_loss: 159.2536WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 24853.26325\n",
      "111958/111958 [==============================] - 2965s 26ms/step - batch: 55978.5000 - size: 1.0000 - loss: 4175.1268 - vae_r_loss: 4015.8733 - vae_kl_loss: 159.2533 - val_loss: 25817.9566 - val_vae_r_loss: 25692.9785 - val_vae_kl_loss: 125.1862\n",
      "Epoch 18/100000\n",
      "111957/111958 [============================>.] - ETA: 0s - batch: 55978.0000 - size: 1.0000 - loss: 4104.3132 - vae_r_loss: 3945.9727 - vae_kl_loss: 158.3462WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 24853.26325\n",
      "111958/111958 [==============================] - 2985s 27ms/step - batch: 55978.5000 - size: 1.0000 - loss: 4104.2922 - vae_r_loss: 3945.9514 - vae_kl_loss: 158.3464 - val_loss: 24855.2916 - val_vae_r_loss: 24732.8340 - val_vae_kl_loss: 122.4800\n",
      "Epoch 19/100000\n",
      "111957/111958 [============================>.] - ETA: 0s - batch: 55978.0000 - size: 1.0000 - loss: 4087.1330 - vae_r_loss: 3930.1345 - vae_kl_loss: 156.9737WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 24853.26325\n",
      "111958/111958 [==============================] - 2973s 27ms/step - batch: 55978.5000 - size: 1.0000 - loss: 4087.1371 - vae_r_loss: 3930.1389 - vae_kl_loss: 156.9735 - val_loss: 25330.2900 - val_vae_r_loss: 25207.4609 - val_vae_kl_loss: 122.8080\n",
      "Epoch 20/100000\n",
      "111957/111958 [============================>.] - ETA: 0s - batch: 55978.0000 - size: 1.0000 - loss: 4004.0812 - vae_r_loss: 3846.6865 - vae_kl_loss: 157.3865WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 24853.26325\n",
      "111958/111958 [==============================] - 2981s 27ms/step - batch: 55978.5000 - size: 1.0000 - loss: 4004.2524 - vae_r_loss: 3846.8579 - vae_kl_loss: 157.3861 - val_loss: 25692.6870 - val_vae_r_loss: 25569.6758 - val_vae_kl_loss: 122.9569\n",
      "Epoch 21/100000\n",
      "111958/111958 [==============================] - ETA: 0s - batch: 55978.5000 - size: 1.0000 - loss: 4089.3739 - vae_r_loss: 3931.8965 - vae_kl_loss: 157.4943WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 24853.26325\n",
      "111958/111958 [==============================] - 3001s 27ms/step - batch: 55978.5000 - size: 1.0000 - loss: 4089.3739 - vae_r_loss: 3931.8965 - vae_kl_loss: 157.4943 - val_loss: 25722.0101 - val_vae_r_loss: 25598.4199 - val_vae_kl_loss: 123.5106\n",
      "Epoch 22/100000\n",
      "111956/111958 [============================>.] - ETA: 0s - batch: 55977.5000 - size: 1.0000 - loss: 4043.0635 - vae_r_loss: 3885.5161 - vae_kl_loss: 157.5547WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 24853.26325\n",
      "111958/111958 [==============================] - 2981s 27ms/step - batch: 55978.5000 - size: 1.0000 - loss: 4043.0214 - vae_r_loss: 3885.4739 - vae_kl_loss: 157.5548 - val_loss: 25142.9162 - val_vae_r_loss: 25018.5742 - val_vae_kl_loss: 124.2807\n",
      "Epoch 23/100000\n",
      "111956/111958 [============================>.] - ETA: 0s - batch: 55977.5000 - size: 1.0000 - loss: 3951.4399 - vae_r_loss: 3793.8818 - vae_kl_loss: 157.5905WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 24853.26325\n",
      "111958/111958 [==============================] - 2987s 27ms/step - batch: 55978.5000 - size: 1.0000 - loss: 3951.5469 - vae_r_loss: 3793.9893 - vae_kl_loss: 157.5899 - val_loss: 25512.4669 - val_vae_r_loss: 25394.0645 - val_vae_kl_loss: 118.5523\n",
      "Epoch 24/100000\n",
      "111957/111958 [============================>.] - ETA: 0s - batch: 55978.0000 - size: 1.0000 - loss: 4082.1756 - vae_r_loss: 3924.9656 - vae_kl_loss: 157.2005WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 24853.26325\n",
      "111958/111958 [==============================] - 3004s 27ms/step - batch: 55978.5000 - size: 1.0000 - loss: 4082.2927 - vae_r_loss: 3925.0825 - vae_kl_loss: 157.2005 - val_loss: 24871.8713 - val_vae_r_loss: 24749.4805 - val_vae_kl_loss: 122.4191\n",
      "Epoch 25/100000\n",
      "111956/111958 [============================>.] - ETA: 0s - batch: 55977.5000 - size: 1.0000 - loss: 4165.1900 - vae_r_loss: 4007.8623 - vae_kl_loss: 157.3369WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 24853.26325\n",
      "111958/111958 [==============================] - 2992s 27ms/step - batch: 55978.5000 - size: 1.0000 - loss: 4165.1358 - vae_r_loss: 4007.8081 - vae_kl_loss: 157.3371 - val_loss: 25196.8513 - val_vae_r_loss: 25072.4746 - val_vae_kl_loss: 124.4486\n",
      "Epoch 26/100000\n",
      "111956/111958 [============================>.] - ETA: 0s - batch: 55977.5000 - size: 1.0000 - loss: 4111.9784 - vae_r_loss: 3955.0857 - vae_kl_loss: 156.9044WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 24853.26325\n",
      "111958/111958 [==============================] - 2985s 27ms/step - batch: 55978.5000 - size: 1.0000 - loss: 4111.9198 - vae_r_loss: 3955.0273 - vae_kl_loss: 156.9042 - val_loss: 24896.8505 - val_vae_r_loss: 24776.4023 - val_vae_kl_loss: 120.3370\n",
      "Epoch 27/100000\n",
      "111957/111958 [============================>.] - ETA: 0s - batch: 55978.0000 - size: 1.0000 - loss: 4243.0681 - vae_r_loss: 4085.5752 - vae_kl_loss: 157.4927WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 24853.26325\n",
      "111958/111958 [==============================] - 2992s 27ms/step - batch: 55978.5000 - size: 1.0000 - loss: 4243.0468 - vae_r_loss: 4085.5537 - vae_kl_loss: 157.4927 - val_loss: 25677.8888 - val_vae_r_loss: 25552.2305 - val_vae_kl_loss: 125.6704\n",
      "Epoch 28/100000\n",
      "111957/111958 [============================>.] - ETA: 0s - batch: 55978.0000 - size: 1.0000 - loss: 4073.6440 - vae_r_loss: 3916.8357 - vae_kl_loss: 156.7698WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 24853.26325\n",
      "111958/111958 [==============================] - 3015s 27ms/step - batch: 55978.5000 - size: 1.0000 - loss: 4073.6155 - vae_r_loss: 3916.8076 - vae_kl_loss: 156.7696 - val_loss: 25528.6402 - val_vae_r_loss: 25405.3691 - val_vae_kl_loss: 123.2440\n",
      "Epoch 29/100000\n",
      "111957/111958 [============================>.] - ETA: 0s - batch: 55978.0000 - size: 1.0000 - loss: 4089.3352 - vae_r_loss: 3932.7188 - vae_kl_loss: 156.6288WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 24853.26325\n",
      "111958/111958 [==============================] - 2980s 27ms/step - batch: 55978.5000 - size: 1.0000 - loss: 4089.3447 - vae_r_loss: 3932.7280 - vae_kl_loss: 156.6293 - val_loss: 25328.0998 - val_vae_r_loss: 25203.5918 - val_vae_kl_loss: 124.5171\n",
      "Epoch 30/100000\n",
      "111957/111958 [============================>.] - ETA: 0s - batch: 55978.0000 - size: 1.0000 - loss: 4124.2059 - vae_r_loss: 3967.1482 - vae_kl_loss: 157.0467WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 24853.26325\n",
      "111958/111958 [==============================] - 3026s 27ms/step - batch: 55978.5000 - size: 1.0000 - loss: 4124.2108 - vae_r_loss: 3967.1531 - vae_kl_loss: 157.0468 - val_loss: 25386.6987 - val_vae_r_loss: 25260.0293 - val_vae_kl_loss: 126.7355\n",
      "Epoch 31/100000\n",
      "111955/111958 [============================>.] - ETA: 0s - batch: 55977.0000 - size: 1.0000 - loss: 4240.0766 - vae_r_loss: 4082.0107 - vae_kl_loss: 158.0266WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 24853.26325\n",
      "111958/111958 [==============================] - 2999s 27ms/step - batch: 55978.5000 - size: 1.0000 - loss: 4240.0400 - vae_r_loss: 4081.9741 - vae_kl_loss: 158.0263 - val_loss: 25313.0432 - val_vae_r_loss: 25184.9219 - val_vae_kl_loss: 128.0890\n",
      "Epoch 32/100000\n",
      "111958/111958 [==============================] - ETA: 0s - batch: 55978.5000 - size: 1.0000 - loss: 4210.1156 - vae_r_loss: 4052.8638 - vae_kl_loss: 157.3053WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 24853.26325\n",
      "111958/111958 [==============================] - 3061s 27ms/step - batch: 55978.5000 - size: 1.0000 - loss: 4210.1156 - vae_r_loss: 4052.8638 - vae_kl_loss: 157.3053 - val_loss: 25207.1852 - val_vae_r_loss: 25084.5566 - val_vae_kl_loss: 122.6052\n",
      "Epoch 33/100000\n",
      "111956/111958 [============================>.] - ETA: 0s - batch: 55977.5000 - size: 1.0000 - loss: 4115.3460 - vae_r_loss: 3957.8455 - vae_kl_loss: 157.5198WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 24853.26325\n",
      "111958/111958 [==============================] - 2974s 27ms/step - batch: 55978.5000 - size: 1.0000 - loss: 4115.3343 - vae_r_loss: 3957.8335 - vae_kl_loss: 157.5197 - val_loss: 25240.0522 - val_vae_r_loss: 25117.7715 - val_vae_kl_loss: 122.4396\n",
      "Epoch 34/100000\n",
      "111957/111958 [============================>.] - ETA: 0s - batch: 55978.0000 - size: 1.0000 - loss: 4227.3109 - vae_r_loss: 4070.0959 - vae_kl_loss: 157.1942WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 24853.26325\n",
      "111958/111958 [==============================] - 3050s 27ms/step - batch: 55978.5000 - size: 1.0000 - loss: 4227.2866 - vae_r_loss: 4070.0720 - vae_kl_loss: 157.1940 - val_loss: 25430.3578 - val_vae_r_loss: 25308.4238 - val_vae_kl_loss: 121.8687\n",
      "Epoch 35/100000\n",
      "111956/111958 [============================>.] - ETA: 0s - batch: 55977.5000 - size: 1.0000 - loss: 4257.8678 - vae_r_loss: 4100.6147 - vae_kl_loss: 157.2509WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 24853.26325\n",
      "111958/111958 [==============================] - 3141s 28ms/step - batch: 55978.5000 - size: 1.0000 - loss: 4257.8203 - vae_r_loss: 4100.5669 - vae_kl_loss: 157.2514 - val_loss: 25777.9411 - val_vae_r_loss: 25653.4922 - val_vae_kl_loss: 124.4089\n",
      "Epoch 36/100000\n",
      "111957/111958 [============================>.] - ETA: 0s - batch: 55978.0000 - size: 1.0000 - loss: 4143.3724 - vae_r_loss: 3986.9436 - vae_kl_loss: 156.4377WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 24853.26325\n",
      "111958/111958 [==============================] - 3064s 27ms/step - batch: 55978.5000 - size: 1.0000 - loss: 4143.3547 - vae_r_loss: 3986.9260 - vae_kl_loss: 156.4377 - val_loss: 25144.3906 - val_vae_r_loss: 25021.3945 - val_vae_kl_loss: 123.0093\n",
      "Epoch 37/100000\n",
      "111958/111958 [==============================] - ETA: 0s - batch: 55978.5000 - size: 1.0000 - loss: 4298.9117 - vae_r_loss: 4141.5664 - vae_kl_loss: 157.3244WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 24853.26325\n",
      "111958/111958 [==============================] - 3015s 27ms/step - batch: 55978.5000 - size: 1.0000 - loss: 4298.9117 - vae_r_loss: 4141.5664 - vae_kl_loss: 157.3244 - val_loss: 25606.3258 - val_vae_r_loss: 25477.6504 - val_vae_kl_loss: 128.6576\n",
      "Epoch 38/100000\n",
      "111958/111958 [==============================] - ETA: 0s - batch: 55978.5000 - size: 1.0000 - loss: 4189.2031 - vae_r_loss: 4032.4016 - vae_kl_loss: 156.7862WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 24853.26325\n",
      "111958/111958 [==============================] - 3072s 27ms/step - batch: 55978.5000 - size: 1.0000 - loss: 4189.2031 - vae_r_loss: 4032.4016 - vae_kl_loss: 156.7862 - val_loss: 26217.6585 - val_vae_r_loss: 26090.4824 - val_vae_kl_loss: 127.1542\n",
      "Epoch 39/100000\n",
      "111958/111958 [==============================] - ETA: 0s - batch: 55978.5000 - size: 1.0000 - loss: 4290.3920 - vae_r_loss: 4132.3252 - vae_kl_loss: 158.0464WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 24853.26325\n",
      "111958/111958 [==============================] - 3027s 27ms/step - batch: 55978.5000 - size: 1.0000 - loss: 4290.3920 - vae_r_loss: 4132.3252 - vae_kl_loss: 158.0464 - val_loss: 25348.7746 - val_vae_r_loss: 25225.0586 - val_vae_kl_loss: 123.8925\n",
      "Epoch 40/100000\n",
      "111958/111958 [==============================] - ETA: 0s - batch: 55978.5000 - size: 1.0000 - loss: 4233.3998 - vae_r_loss: 4075.9382 - vae_kl_loss: 157.4582WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 24853.26325\n",
      "111958/111958 [==============================] - 3038s 27ms/step - batch: 55978.5000 - size: 1.0000 - loss: 4233.3998 - vae_r_loss: 4075.9382 - vae_kl_loss: 157.4582 - val_loss: 25910.3153 - val_vae_r_loss: 25781.7637 - val_vae_kl_loss: 128.6466\n",
      "Epoch 41/100000\n",
      "111958/111958 [==============================] - ETA: 0s - batch: 55978.5000 - size: 1.0000 - loss: 4171.3095 - vae_r_loss: 4013.3962 - vae_kl_loss: 157.9029WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 24853.26325\n",
      "111958/111958 [==============================] - 2979s 27ms/step - batch: 55978.5000 - size: 1.0000 - loss: 4171.3095 - vae_r_loss: 4013.3962 - vae_kl_loss: 157.9029 - val_loss: 25738.8649 - val_vae_r_loss: 25612.6211 - val_vae_kl_loss: 126.3671\n",
      "Epoch 42/100000\n",
      "111958/111958 [==============================] - ETA: 0s - batch: 55978.5000 - size: 1.0000 - loss: 4098.7278 - vae_r_loss: 3942.6106 - vae_kl_loss: 156.1528WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 24853.26325\n",
      "111958/111958 [==============================] - 3012s 27ms/step - batch: 55978.5000 - size: 1.0000 - loss: 4098.7278 - vae_r_loss: 3942.6106 - vae_kl_loss: 156.1528 - val_loss: 25244.4361 - val_vae_r_loss: 25118.9688 - val_vae_kl_loss: 125.5773\n",
      "Epoch 43/100000\n",
      "111957/111958 [============================>.] - ETA: 0s - batch: 55978.0000 - size: 1.0000 - loss: 4374.1532 - vae_r_loss: 4216.0322 - vae_kl_loss: 158.0930WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 24853.26325\n",
      "111958/111958 [==============================] - 3013s 27ms/step - batch: 55978.5000 - size: 1.0000 - loss: 4374.1659 - vae_r_loss: 4216.0444 - vae_kl_loss: 158.0936 - val_loss: 25312.2783 - val_vae_r_loss: 25184.8262 - val_vae_kl_loss: 127.5837\n",
      "Epoch 44/100000\n",
      "111956/111958 [============================>.] - ETA: 0s - batch: 55977.5000 - size: 1.0000 - loss: 4281.6609 - vae_r_loss: 4124.8657 - vae_kl_loss: 156.7630WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 24853.26325\n",
      "111958/111958 [==============================] - 3039s 27ms/step - batch: 55978.5000 - size: 1.0000 - loss: 4282.0674 - vae_r_loss: 4125.2720 - vae_kl_loss: 156.7630 - val_loss: 25292.6940 - val_vae_r_loss: 25161.8594 - val_vae_kl_loss: 130.8611\n",
      "Epoch 45/100000\n",
      "111956/111958 [============================>.] - ETA: 0s - batch: 55977.5000 - size: 1.0000 - loss: 4350.8209 - vae_r_loss: 4192.8428 - vae_kl_loss: 157.9822WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 24853.26325\n",
      "111958/111958 [==============================] - 3038s 27ms/step - batch: 55978.5000 - size: 1.0000 - loss: 4350.7600 - vae_r_loss: 4192.7817 - vae_kl_loss: 157.9823 - val_loss: 25470.0042 - val_vae_r_loss: 25342.3066 - val_vae_kl_loss: 127.5909\n",
      "Epoch 46/100000\n",
      "111956/111958 [============================>.] - ETA: 0s - batch: 55977.5000 - size: 1.0000 - loss: 4298.5326 - vae_r_loss: 4141.3921 - vae_kl_loss: 157.1293WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 24853.26325\n",
      "111958/111958 [==============================] - 3038s 27ms/step - batch: 55978.5000 - size: 1.0000 - loss: 4298.7471 - vae_r_loss: 4141.6069 - vae_kl_loss: 157.1287 - val_loss: 25545.0070 - val_vae_r_loss: 25417.7949 - val_vae_kl_loss: 127.0173\n",
      "Epoch 47/100000\n",
      "111957/111958 [============================>.] - ETA: 0s - batch: 55978.0000 - size: 1.0000 - loss: 4383.9349 - vae_r_loss: 4226.8462 - vae_kl_loss: 157.0798WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 24853.26325\n",
      "111958/111958 [==============================] - 3121s 28ms/step - batch: 55978.5000 - size: 1.0000 - loss: 4383.9605 - vae_r_loss: 4226.8706 - vae_kl_loss: 157.0806 - val_loss: 25598.5401 - val_vae_r_loss: 25468.3340 - val_vae_kl_loss: 130.0956\n",
      "Epoch 48/100000\n",
      "111957/111958 [============================>.] - ETA: 0s - batch: 55978.0000 - size: 1.0000 - loss: 4426.4808 - vae_r_loss: 4269.3589 - vae_kl_loss: 157.1486WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 24853.26325\n",
      "111958/111958 [==============================] - 3160s 28ms/step - batch: 55978.5000 - size: 1.0000 - loss: 4426.4580 - vae_r_loss: 4269.3354 - vae_kl_loss: 157.1490 - val_loss: 25237.1599 - val_vae_r_loss: 25111.7188 - val_vae_kl_loss: 125.4230\n",
      "Epoch 49/100000\n",
      "111957/111958 [============================>.] - ETA: 0s - batch: 55978.0000 - size: 1.0000 - loss: 4504.6242 - vae_r_loss: 4346.6895 - vae_kl_loss: 157.9200WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 24853.26325\n",
      "111958/111958 [==============================] - 3090s 28ms/step - batch: 55978.5000 - size: 1.0000 - loss: 4504.5981 - vae_r_loss: 4346.6631 - vae_kl_loss: 157.9199 - val_loss: 25745.9865 - val_vae_r_loss: 25618.1660 - val_vae_kl_loss: 127.7721\n",
      "Epoch 50/100000\n",
      "111958/111958 [==============================] - ETA: 0s - batch: 55978.5000 - size: 1.0000 - loss: 4412.3248 - vae_r_loss: 4254.6230 - vae_kl_loss: 157.6979WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 24853.26325\n",
      "111958/111958 [==============================] - 3023s 27ms/step - batch: 55978.5000 - size: 1.0000 - loss: 4412.3248 - vae_r_loss: 4254.6230 - vae_kl_loss: 157.6979 - val_loss: 25603.5532 - val_vae_r_loss: 25477.5684 - val_vae_kl_loss: 126.1591\n",
      "Epoch 51/100000\n",
      "111957/111958 [============================>.] - ETA: 0s - batch: 55978.0000 - size: 1.0000 - loss: 4466.3693 - vae_r_loss: 4307.9897 - vae_kl_loss: 158.3742WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 24853.26325\n",
      "111958/111958 [==============================] - 3951s 35ms/step - batch: 55978.5000 - size: 1.0000 - loss: 4466.3428 - vae_r_loss: 4307.9634 - vae_kl_loss: 158.3744 - val_loss: 25478.8750 - val_vae_r_loss: 25352.8066 - val_vae_kl_loss: 125.9881\n",
      "Epoch 52/100000\n",
      "111957/111958 [============================>.] - ETA: 0s - batch: 55978.0000 - size: 1.0000 - loss: 4378.6810 - vae_r_loss: 4221.3022 - vae_kl_loss: 157.3612WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 24853.26325\n",
      "111958/111958 [==============================] - 4249s 38ms/step - batch: 55978.5000 - size: 1.0000 - loss: 4378.6547 - vae_r_loss: 4221.2759 - vae_kl_loss: 157.3612 - val_loss: 25402.4738 - val_vae_r_loss: 25276.9688 - val_vae_kl_loss: 125.4951\n",
      "Epoch 53/100000\n",
      " 31966/111958 [=======>......................] - ETA: 43:49 - batch: 15982.5000 - size: 1.0000 - loss: 4562.3850 - vae_r_loss: 4403.5317 - vae_kl_loss: 158.8467"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-6b2bd2fd24a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m48\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m                         )\n",
      "\u001b[0;32m~/anaconda3/envs/vae/lib/python3.6/site-packages/keras/engine/training_v1.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    794\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 796\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/vae/lib/python3.6/site-packages/keras/engine/training_generator_v1.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/vae/lib/python3.6/site-packages/keras/engine/training_generator_v1.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/vae/lib/python3.6/site-packages/keras/engine/training_v1.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1074\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_sample_weight_modes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1076\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1077\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/vae/lib/python3.6/site-packages/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   4030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4031\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 4032\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   4033\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4034\u001b[0m     output_structure = tf.nest.pack_sequence_as(\n",
      "\u001b[0;32m~/anaconda3/envs/vae/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1478\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1479\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1480\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1481\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1482\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "## Train the VAE\n",
    "\"\"\"\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "\n",
    "        # checkpoint\n",
    "callbacks = []\n",
    "#weight_filename = self.data_path + '_' + Config.config_yaml_name \\\n",
    "#    + '_N' + str(config['network_type']) + '_ckpt'\n",
    "model_ckpt_name = \"vae_kdh\"\n",
    "checkpoint = ModelCheckpoint(model_ckpt_name +'_ckpt.{epoch:02d}-{val_loss:.3f}.h5',\n",
    "                                monitor='val_loss', \n",
    "                                verbose=1, save_best_only=True, mode='min')\n",
    "callbacks.append(checkpoint)\n",
    "\n",
    "# early stopping\n",
    "patience = 5000\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=patience, \n",
    "                            verbose=1, mode='min')\n",
    "# callbacks.append(earlystop)\n",
    "\n",
    "# validation_steps = len(valid_data)//config['batch_size']\n",
    "# print(validation_steps)\n",
    "train_hist = VarAE.fit( train_generator,  \n",
    "                        steps_per_epoch=len(train_data), \n",
    "                        epochs=100000, \n",
    "                        validation_data=valid_generator,\n",
    "                        validation_steps=len(valid_data),\n",
    "                        shuffle=False,\n",
    "                        verbose=1, \n",
    "                        callbacks=callbacks, \n",
    "                        use_multiprocessing=True,\n",
    "                        workers=48\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import array_to_img\n",
    "def plot_latent_space(trained_decoder, pimage, n=50, figsize=15):\n",
    "\n",
    "\n",
    "    image = pimage[Config.data_collection['image_crop_y1']:Config.data_collection['image_crop_y2'],\n",
    "                Config.data_collection['image_crop_x1']:Config.data_collection['image_crop_x2']]\n",
    "    image = cv2.resize(image, \n",
    "                        (config['input_image_width'],\n",
    "                        config['input_image_height']))\n",
    "    image = np.array(image).astype(\"float32\")/255.0\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    plt.show()\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    # vel = np.array()\n",
    "\n",
    "\n",
    "    X_tvel = np.array([14.5])\n",
    "    X_tvel = np.expand_dims(X_tvel, axis=0)\n",
    "    X_tstr = np.array([0.3])\n",
    "    X_tvel = np.expand_dims(X_tstr, axis=0)\n",
    "    X_ttime = np.array([0.1])\n",
    "    X_tvel = np.expand_dims(X_ttime, axis=0)\n",
    "    # print(X_tvel.shape)\n",
    "    X_train = [image, X_tstr, X_tvel, X_ttime]\n",
    "\n",
    "    x_encoded = VarAE.predict(X_train)*255.0\n",
    "    # print(x_encoded)\n",
    "    x_encoded_out = np.squeeze(x_encoded,axis=0)\n",
    "    # pred_image = np.zeros([160,160,3], np.uint8)\n",
    "    pred_image = array_to_img(x_encoded_out)\n",
    "    pred_image = np.array(pred_image)\n",
    "    pred_image = cv2.cvtColor(pred_image, cv2.COLOR_BGR2RGB)\n",
    "    # pred_img = cv2.cvtColor(pred_image, cv2.COLOR_BGR2RGB)\n",
    "    # print(type(pred_img))\n",
    "    # print(pred_image.shape)\n",
    "    plt.imshow(pred_image)\n",
    "    # plt.imshow(cv2.cvtColor(pred_image, cv2.COLOR_BGR2RGB))\n",
    "    plt.show()\n",
    "\n",
    "    origin_image_name = '/home2/kdh/vae/vae_test/2022-12-08-14-30-12-332957.jpg'\n",
    "    cv2.imwrite('/')\n",
    "    # plt.xticks(pixel_range, sample_range_x)\n",
    "    # plt.yticks(pixel_range, sample_range_y)\n",
    "    # plt.xlabel(\"z[0]\")\n",
    "    # plt.ylabel(\"z[1]\")\n",
    "\n",
    "# test_img = cv2.imread('/home2/kdh/vae/2022-12-08-14-25-28/2022-12-08-14-30-12-332957.jpg')\n",
    "# plot_latent_space(decoder, test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import array_to_img\n",
    "def save_latent_space(trained_decoder, valid_data, n=50, figsize=15):\n",
    "        \n",
    "    for image_name, velocity, measurement, tar_image_name, tar_steering_angle, tar_vel, tar_time in valid_data:\n",
    "        # self.data.image_names, self.data.velocities, self.data.measurements, \n",
    "        # self.data.tar_image_names, self.data.tar_steering_angle, self.data.tar_vel, self.data.tar_time\n",
    "        \n",
    "        image_path = data_path + '/' + image_name\n",
    "        # print(data_path, tar_image_name)\n",
    "        tar_image_path = data_path + '/' + tar_image_name\n",
    "        image = cv2.imread(image_path)\n",
    "        tar_image = cv2.imread(tar_image_path)\n",
    "\n",
    "        # if collected data is not cropped then crop here\n",
    "        # otherwise do not crop.\n",
    "        image = image[Config.data_collection['image_crop_y1']:Config.data_collection['image_crop_y2'],\n",
    "                    Config.data_collection['image_crop_x1']:Config.data_collection['image_crop_x2']]\n",
    "        image = cv2.resize(image, \n",
    "                            (config['input_image_width'],\n",
    "                            config['input_image_height']))\n",
    "        tar_image = tar_image[Config.data_collection['image_crop_y1']:Config.data_collection['image_crop_y2'],\n",
    "                    Config.data_collection['image_crop_x1']:Config.data_collection['image_crop_x2']]\n",
    "        tar_image = cv2.resize(tar_image, \n",
    "                            (config['input_image_width'],\n",
    "                            config['input_image_height']))\n",
    "\n",
    "        cv2.imwrite('/home2/kdh/vae/vae_test/'+image_name, image)\n",
    "        image = np.array(image).astype(\"float32\")/255.0\n",
    "        # image = image_process.process(image)\n",
    "        steering_angle, throttle, brake = measurement\n",
    "        \n",
    "        image = np.expand_dims(image, axis=0)\n",
    "        X_tvel = np.array([velocity])\n",
    "        X_tvel = np.expand_dims(X_tvel, axis=0)\n",
    "        X_tstr = np.array([steering_angle])\n",
    "        X_tstr = np.expand_dims(X_tstr, axis=0)\n",
    "        X_ttime = np.array([tar_time])\n",
    "        X_ttime = np.expand_dims(X_ttime, axis=0)\n",
    "        \n",
    "        X_train = [image, X_tstr, X_tvel, X_ttime]\n",
    "\n",
    "        x_encoded = VarAE.predict(X_train)*255.0\n",
    "        x_encoded_out = np.squeeze(x_encoded,axis=0)\n",
    "        pred_image = array_to_img(x_encoded_out)\n",
    "        pred_image = np.array(pred_image)\n",
    "        # pred_image = cv2.cvtColor(pred_image)\n",
    "        # plt.imshow(pred_image)\n",
    "        # plt.show()\n",
    "\n",
    "        # image_name = '/home2/kdh/vae/vae_test/'+image_name\n",
    "        pred_image_name = '/home2/kdh/vae/vae_test/'+image_name[:-4]+'_'+str(int(steering_angle*1000))+'_'+str(int(velocity))+'_'+str(tar_time)[:4]+'.jpg'\n",
    "        gt_tar_image_name = '/home2/kdh/vae/vae_test/'+image_name[:-4]+'_'+str(int(steering_angle*1000))+'_'+str(int(velocity))+'_'+str(tar_time)[:4]+'_gt.jpg'\n",
    "        cv2.imwrite(pred_image_name, pred_image)\n",
    "        cv2.imwrite(gt_tar_image_name, tar_image)\n",
    "\n",
    "        # print(pred_image_name,' done')\n",
    "        # cv2.imwrite('/')\n",
    "\n",
    "\n",
    "# save_latent_space(decoder, valid_data[3423:4422])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import array_to_img\n",
    "def save_latent_space_diff_steer(trained_decoder, pimage, save_image_name, n=50, figsize=15):\n",
    "\n",
    "\n",
    "    image = pimage[Config.data_collection['image_crop_y1']:Config.data_collection['image_crop_y2'],\n",
    "                Config.data_collection['image_crop_x1']:Config.data_collection['image_crop_x2']]\n",
    "    image = cv2.resize(image, \n",
    "                        (config['input_image_width'],\n",
    "                        config['input_image_height']))\n",
    "    cv2.imwrite('/home2/kdh/vae/vae_diff_steer_test/'+save_image_name+'.jpg', image)\n",
    "    image = np.array(image).astype(\"float32\")/255.0\n",
    "    # plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    # plt.show()\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    # vel = np.array()\n",
    "\n",
    "\n",
    "    X_tvel = np.array([14.85])\n",
    "    X_tvel = np.expand_dims(X_tvel, axis=0)\n",
    "    X_ttime = np.array([0.8])\n",
    "    X_ttime = np.expand_dims(X_ttime, axis=0)\n",
    "    # print(X_tvel.shape)\n",
    "    for s in np.arange(0, 0.8, 0.0125):\n",
    "        X_tstr = np.array([s-0.4])\n",
    "        X_tstr = np.expand_dims(X_tstr, axis=0)\n",
    "        X_train = [image, X_tstr, X_tvel, X_ttime]\n",
    "\n",
    "        x_encoded = VarAE.predict(X_train)*255.0\n",
    "        x_encoded_out = np.squeeze(x_encoded,axis=0)\n",
    "        pred_image = array_to_img(x_encoded_out)\n",
    "        pred_image = np.array(pred_image)\n",
    "        # pred_image = cv2.cvtColor(pred_image, cv2.COLOR_BGR2RGB)\n",
    "        s_str = \"{:.3f}\".format(s)\n",
    "        pred_image_name = ('/home2/kdh/vae/vae_diff_steer_test/'\n",
    "                            +save_image_name\n",
    "                            +'_'+str(s_str)\n",
    "                            +'.jpg'\n",
    "        )\n",
    "        cv2.imwrite(pred_image_name, pred_image)\n",
    "    \n",
    "        \n",
    "\n",
    "# test_img = cv2.imread('/home2/kdh/vae/2022-12-08-14-25-28/2022-12-08-14-35-43-708754.jpg')\n",
    "# save_latent_space_diff_steer(decoder, test_img, '2022-12-08-14-35-43-708754')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('vae')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "06e874fbf5df5387c03d37e60b157d9aec9842d39ece54c5a1a7aaa8e8f1e8af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
